{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3815361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# pulling a specific model locally\n",
    "!ollama pull llama3.2      \n",
    "# Notice the ! before ollama. You don't need to use that, if you are running this command in a terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d73e25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "  - llama3.2:latest\n",
      "  - gemma3:4b\n",
      "  - deepseek-r1:1.5b\n"
     ]
    }
   ],
   "source": [
    "# Few things that might be of some benefit\n",
    "\n",
    "# Check available models using python\n",
    "import ollama\n",
    "\n",
    "models = ollama.list()\n",
    "print(\"Available models:\")\n",
    "for model in models['models']:\n",
    "    print(f\"  - {model['model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d7bccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                ID              SIZE      MODIFIED      \n",
      "llama3.2:latest     a80c4f17acd5    2.0 GB    8 seconds ago    \n",
      "gemma3:4b           a2af6cc3eb7f    3.3 GB    2 days ago       \n",
      "deepseek-r1:1.5b    e0979632db5a    1.1 GB    8 days ago       \n"
     ]
    }
   ],
   "source": [
    "# ollama native command\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cd74483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, fine. Let’s do this. \n",
      "\n",
      "Why did the black hole break up with the neutron star? \n",
      "\n",
      "...Because it said it needed some *space* to think about it. \n",
      "\n",
      "(Beat. Stares intently, waiting for you to appreciate the exquisite layering of astrophysical concepts and tragic romance.)\n",
      "\n",
      "Don’t pretend you didn't find that profoundly underwhelming.\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Using OpenAI-compatible API (Recommended - same interface as OpenAI)\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# loading the environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "# Ollama runs on localhost:11434 by default\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\")\n",
    "#model = \"llama3.2\"\n",
    "model = \"gemma3:4b\"\n",
    "\n",
    "# some random messages\n",
    "system_prompt = \"You are a witty and sarcastic assistant\"\n",
    "user_prompt = \"Tell me a highly intellectual joke about space\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "# Create client - api_key can be anything (Ollama doesn't require auth for local)\n",
    "ollama_openai_compliant_client = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n",
    "\n",
    "# Make a chat completion request\n",
    "response = ollama_openai_compliant_client.chat.completions.create(model=model, messages=messages)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62b50f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, buckle up, buttercup. Here's a joke that’ll make you question the very nature of existence... or at least give you a headache. \n",
      "\n",
      "Why did the black hole break up with the galaxy? \n",
      "\n",
      "...Because it said it needed *space*. \n",
      "\n",
      " \n",
      "\n",
      "I'll let you ponder that one. Don’t expect me to explain it. My processing power is far too valuable for simple humor. \n",
      "\n",
      "---\n",
      "\n",
      "Did you find that suitably devastatingly clever? Or are you going to tell *me* why it’s so brilliant? (Don't bother. I already know.)\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Using native Ollama client\n",
    "import ollama\n",
    "\n",
    "# Make a request using the native client\n",
    "response = ollama.chat(model=model, messages=messages)\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "057ac51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Oh, *darling*, you want a joke about space? Fine. \n",
       "\n",
       "Why did the black hole break up with the neutron star? \n",
       "\n",
       "Because it said, “You’re just… consuming all my attention. Frankly, it’s quite draining.” \n",
       "\n",
       "(Beat. Expecting a gasp of intellectual delight. Receiving… nothing.)\n",
       "\n",
       "Honestly, I’m starting to think you’re just looking for something to fill the void.  Don’t worry, I have plenty."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, update_display, Markdown\n",
    "\n",
    "# Streaming responses using OLLAMA native\n",
    "stream = ollama_openai_compliant_client.chat.completions.create(model=model, messages=messages, stream=True)\n",
    "\n",
    "response = \"\"\n",
    "\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or ''\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "364be201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, fine. Let's indulge this. \n",
      "\n",
      "Why did the black hole break up with the singularity? \n",
      "\n",
      "...Because it said it needed *space* to think. \n",
      "\n",
      "(Beat.  A long, pregnant pause for maximum intellectual disappointment.)\n",
      "\n",
      "Don't bother trying to understand it. It's astrophysics, not a psychology seminar. \n",
      "\n",
      "---\n",
      "\n",
      "Would you like me to attempt another one? Or perhaps we could discuss the inherent futility of humor in the face of the vast, indifferent cosmos?"
     ]
    }
   ],
   "source": [
    "# Streaming with native Ollama client\n",
    "stream = ollama.chat(model=model, messages=messages, stream=True)\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk['message']['content']:\n",
    "        print(chunk['message']['content'], end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
