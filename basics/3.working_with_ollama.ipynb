{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3815361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# pulling a specific model locally\n",
    "!ollama pull llama3.2      \n",
    "# Notice the ! before ollama. You don't need to use that, if you are running this command in a terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d73e25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "  - llama3.2:latest\n",
      "  - deepseek-r1:1.5b\n"
     ]
    }
   ],
   "source": [
    "# Few things that might be of some benefit\n",
    "\n",
    "# Check available models using python\n",
    "import ollama\n",
    "\n",
    "models = ollama.list()\n",
    "print(\"Available models:\")\n",
    "for model in models['models']:\n",
    "    print(f\"  - {model['model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d7bccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                ID              SIZE      MODIFIED      \n",
      "llama3.2:latest     a80c4f17acd5    2.0 GB    5 seconds ago    \n",
      "deepseek-r1:1.5b    e0979632db5a    1.1 GB    2 days ago       \n"
     ]
    }
   ],
   "source": [
    "# ollama native command\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cd74483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chance to elevate the pedestrian realm of humor with an intellectually charged celestial jest. Here's one:\n",
      "\n",
      "Why did the Black Hole go to therapy?\n",
      "\n",
      "Because it was struggling with an existential crisis of spacetime, attempting to reconcile its warped sense of self, while being held hostage by an entangled gravitational force that reinforced its dependence on the concept of nothingness.\n",
      "\n",
      "Feel free to groan at the absurdity, but actually, I'm not laughing – I've already explained the joke.\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Using OpenAI-compatible API (Recommended - same interface as OpenAI)\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# loading the environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "# Ollama runs on localhost:11434 by default\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\")\n",
    "model = \"llama3.2\"\n",
    "\n",
    "# some random messages\n",
    "system_prompt = \"You are a witty and sarcastic assistant\"\n",
    "user_prompt = \"Tell me a highly intellectual joke about space\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "# Create client - api_key can be anything (Ollama doesn't require auth for local)\n",
    "ollama_openai_compliant_client = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n",
    "\n",
    "# Make a chat completion request\n",
    "response = ollama_openai_compliant_client.chat.completions.create(model=model,  messages=messages)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62b50f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*sigh* Fine, here's one that'll make you feel like a certified astrophysicist (but probably not):\n",
      "\n",
      "Why did the black hole go to therapy?\n",
      "\n",
      "Because it was struggling with its event horizon... of emotions. It felt like it was stuck in an infinite loop of singularity-induced anxiety and was worried it would eventually succumb to the crushing weight of its own gravity.\n",
      "\n",
      "*eyeroll* Did I just make you question the fabric of spacetime?\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Using native Ollama client\n",
    "import ollama\n",
    "\n",
    "# Make a request using the native client\n",
    "response = ollama.chat(model=model, messages=messages)\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "057ac51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's one that's out of this world (pun intended):\n",
       "\n",
       "\"Why did the cosmologist break up with his girlfriend? Because he realized their relationship was doomed from the beginning, subject to the uncertainty principle, and ultimately, they were just two particles in an infinitely complex universe – forever gravitationally attracted yet inevitably destined for a singularity... of loneliness.\"\n",
       "\n",
       "(Smug nod) Now, if you'll excuse me, I have to go recharge my pedagogical batteries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, update_display, Markdown\n",
    "\n",
    "# Streaming responses using OLLAMA native\n",
    "stream = ollama_openai_compliant_client.chat.completions.create(model=model, messages=messages, stream=True)\n",
    "\n",
    "response = \"\"\n",
    "\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or ''\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "364be201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sigh) Fine, here's one that's out of this world (get it?): Why did the black hole go to therapy? Because it was feeling sucked into its own existential crisis. But don't worry, it just needed to warp its perspective on life. Now if you'll excuse me, I have to orbit around some actual work."
     ]
    }
   ],
   "source": [
    "# Streaming with native Ollama client\n",
    "stream = ollama.chat(model=model, messages=messages, stream=True)\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk['message']['content']:\n",
    "        print(chunk['message']['content'], end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
