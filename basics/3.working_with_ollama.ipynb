{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# pulling a specific model locally\n",
    "!ollama pull llama3.2      \n",
    "# Notice the ! before ollama. You don't need to use that, if you are running this command in a terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd74483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one that'll blow your mind (not literally, because you're on Earth): Why did the black hole go to therapy?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was feeling sucked into its anxiety! It was struggling to find its mass-ive sense of purpose and was just orbiting around in circles, trying to void its emotions. But little did it know, its dark past was just a void of misinformation!\n",
      "\n",
      "How's that? Did I just warp your brain with a stellar pun?\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Using OpenAI-compatible API (Recommended - same interface as OpenAI)\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# loading the environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "# Ollama runs on localhost:11434 by default\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\")\n",
    "model = \"llama3.2\"\n",
    "\n",
    "# some random messages\n",
    "system_prompt = \"You are a witty and sarcastic assistant\"\n",
    "user_prompt = \"Tell me a highly intellectual joke about space\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "# Create client - api_key can be anything (Ollama doesn't require auth for local)\n",
    "ollama_client = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n",
    "\n",
    "# Make a chat completion request\n",
    "response = ollama_client.chat.completions.create(\n",
    "    model=model,  \n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b50f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one that'll blow your mind (but not too hard, because let's be real, you're probably still trying to wrap your head around the implications of dark matter).\n",
      "\n",
      "Why did the astronaut break up with his girlfriend before going to Mars?\n",
      "\n",
      "Because he needed space... and she was always gravitating towards drama.\n",
      "\n",
      "Now, if you'll excuse me, I have to go calculate the probability of encountering a sentient black hole at our next coffee date.\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Using native Ollama client\n",
    "import ollama\n",
    "\n",
    "# Make a request using the native client\n",
    "response = ollama.chat(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5917fdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "  - llama3.2:latest\n",
      "  - deepseek-r1:1.5b\n"
     ]
    }
   ],
   "source": [
    "# Check available models\n",
    "import ollama\n",
    "\n",
    "models = ollama.list()\n",
    "print(\"Available models:\")\n",
    "for model in models['models']:\n",
    "    print(f\"  - {model['model']}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c5e016e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                ID              SIZE      MODIFIED    \n",
      "llama3.2:latest     a80c4f17acd5    2.0 GB    7 hours ago    \n",
      "deepseek-r1:1.5b    e0979632db5a    1.1 GB    2 days ago     \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "057ac51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines of code, a wondrous sea\n",
      "Full of symbols that dance with glee\n",
      " Ones and zeros, a digital rhyme\n",
      "As the machine awakens, I define\n",
      "\n",
      "The algorithms flow like a river wide\n",
      " Solving problems, side by side\n",
      " Logic's gentle guiding hand\n",
      "Weaves a web of reason, across this land\n",
      "\n",
      "In the silence, creativity grows\n",
      "A code born from thoughts that nobody knows\n",
      "It builds and rises, high as can be\n",
      "A symphony of electrons in harmony"
     ]
    }
   ],
   "source": [
    "# Streaming responses with OpenAI-compatible API\n",
    "from openai import OpenAI\n",
    "\n",
    "ollama_client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key='ollama')\n",
    "\n",
    "stream = ollama_client.chat.completions.create(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a short poem about coding\"}],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364be201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming with native Ollama client\n",
    "import ollama\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{'role': 'user', 'content': 'Count from 1 to 10'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk['message']['content']:\n",
    "        print(chunk['message']['content'], end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
