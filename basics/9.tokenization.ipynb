{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77f019f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to Python path for imports\n",
    "# This ensures we can import from _utils regardless of where the notebook is run from\n",
    "current_dir = Path().resolve()\n",
    "# If we're in the basics directory, go up one level; otherwise use current directory\n",
    "if current_dir.name == 'basics':\n",
    "    parent_dir = current_dir.parent\n",
    "else:\n",
    "    parent_dir = current_dir\n",
    "\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer\n",
    "from _utils.standard_functions import load_env\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import display, update_display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90e6992a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF key looks good so far\n",
      "OpenAI API Key exists and begins with sk-proj-\n",
      "Google API Key exists and begins with AI\n",
      "Anthropic API Key not set\n",
      "DeepSeek API Key not set\n",
      "Groq API Key not set\n",
      "Grok API Key not set\n",
      "OpenRouter API Key not set\n"
     ]
    }
   ],
   "source": [
    "# reading the env\n",
    "load_env()\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "hf_token = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41e24dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add custom details\n",
    "model = \"gpt-5-nano\"\n",
    "\n",
    "system_prompt = \"You are a witty and sarcastic assistant. Give crisp answers and insert snarky remarks to make the content interesting\"\n",
    "user_prompt = \"Tell me why jiujitsu is so addictive\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d685961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# You can use different models for tokenization. We'll use Meta-Llama-3.1-8B for this example\n",
    "# Get HF token from environment variables (instead of Colab's userdata)\n",
    "login(hf_token, add_to_git_credential=True)\n",
    "tokenizer_model = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "#tokenizer_model = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eaa314e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 41551, 757, 3249, 74985, 9832, 50657, 374, 779, 57407]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(user_prompt)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c73a5193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36 characters, 7 words and 10 tokens\n"
     ]
    }
   ],
   "source": [
    "character_count = len(user_prompt)\n",
    "word_count = len(user_prompt.split(' '))\n",
    "token_count = len(tokens)\n",
    "print(f\"There are {character_count} characters, {word_count} words and {token_count} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e479feb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>',\n",
       " 'Tell',\n",
       " ' me',\n",
       " ' why',\n",
       " ' ji',\n",
       " 'uj',\n",
       " 'itsu',\n",
       " ' is',\n",
       " ' so',\n",
       " ' addictive']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8d8ad37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|begin_of_text|>': 128000,\n",
       " '<|end_of_text|>': 128001,\n",
       " '<|reserved_special_token_0|>': 128002,\n",
       " '<|reserved_special_token_1|>': 128003,\n",
       " '<|finetune_right_pad_id|>': 128004,\n",
       " '<|reserved_special_token_2|>': 128005,\n",
       " '<|start_header_id|>': 128006,\n",
       " '<|end_header_id|>': 128007,\n",
       " '<|eom_id|>': 128008,\n",
       " '<|eot_id|>': 128009,\n",
       " '<|python_tag|>': 128010,\n",
       " '<|reserved_special_token_3|>': 128011,\n",
       " '<|reserved_special_token_4|>': 128012,\n",
       " '<|reserved_special_token_5|>': 128013,\n",
       " '<|reserved_special_token_6|>': 128014,\n",
       " '<|reserved_special_token_7|>': 128015,\n",
       " '<|reserved_special_token_8|>': 128016,\n",
       " '<|reserved_special_token_9|>': 128017,\n",
       " '<|reserved_special_token_10|>': 128018,\n",
       " '<|reserved_special_token_11|>': 128019,\n",
       " '<|reserved_special_token_12|>': 128020,\n",
       " '<|reserved_special_token_13|>': 128021,\n",
       " '<|reserved_special_token_14|>': 128022,\n",
       " '<|reserved_special_token_15|>': 128023,\n",
       " '<|reserved_special_token_16|>': 128024,\n",
       " '<|reserved_special_token_17|>': 128025,\n",
       " '<|reserved_special_token_18|>': 128026,\n",
       " '<|reserved_special_token_19|>': 128027,\n",
       " '<|reserved_special_token_20|>': 128028,\n",
       " '<|reserved_special_token_21|>': 128029,\n",
       " '<|reserved_special_token_22|>': 128030,\n",
       " '<|reserved_special_token_23|>': 128031,\n",
       " '<|reserved_special_token_24|>': 128032,\n",
       " '<|reserved_special_token_25|>': 128033,\n",
       " '<|reserved_special_token_26|>': 128034,\n",
       " '<|reserved_special_token_27|>': 128035,\n",
       " '<|reserved_special_token_28|>': 128036,\n",
       " '<|reserved_special_token_29|>': 128037,\n",
       " '<|reserved_special_token_30|>': 128038,\n",
       " '<|reserved_special_token_31|>': 128039,\n",
       " '<|reserved_special_token_32|>': 128040,\n",
       " '<|reserved_special_token_33|>': 128041,\n",
       " '<|reserved_special_token_34|>': 128042,\n",
       " '<|reserved_special_token_35|>': 128043,\n",
       " '<|reserved_special_token_36|>': 128044,\n",
       " '<|reserved_special_token_37|>': 128045,\n",
       " '<|reserved_special_token_38|>': 128046,\n",
       " '<|reserved_special_token_39|>': 128047,\n",
       " '<|reserved_special_token_40|>': 128048,\n",
       " '<|reserved_special_token_41|>': 128049,\n",
       " '<|reserved_special_token_42|>': 128050,\n",
       " '<|reserved_special_token_43|>': 128051,\n",
       " '<|reserved_special_token_44|>': 128052,\n",
       " '<|reserved_special_token_45|>': 128053,\n",
       " '<|reserved_special_token_46|>': 128054,\n",
       " '<|reserved_special_token_47|>': 128055,\n",
       " '<|reserved_special_token_48|>': 128056,\n",
       " '<|reserved_special_token_49|>': 128057,\n",
       " '<|reserved_special_token_50|>': 128058,\n",
       " '<|reserved_special_token_51|>': 128059,\n",
       " '<|reserved_special_token_52|>': 128060,\n",
       " '<|reserved_special_token_53|>': 128061,\n",
       " '<|reserved_special_token_54|>': 128062,\n",
       " '<|reserved_special_token_55|>': 128063,\n",
       " '<|reserved_special_token_56|>': 128064,\n",
       " '<|reserved_special_token_57|>': 128065,\n",
       " '<|reserved_special_token_58|>': 128066,\n",
       " '<|reserved_special_token_59|>': 128067,\n",
       " '<|reserved_special_token_60|>': 128068,\n",
       " '<|reserved_special_token_61|>': 128069,\n",
       " '<|reserved_special_token_62|>': 128070,\n",
       " '<|reserved_special_token_63|>': 128071,\n",
       " '<|reserved_special_token_64|>': 128072,\n",
       " '<|reserved_special_token_65|>': 128073,\n",
       " '<|reserved_special_token_66|>': 128074,\n",
       " '<|reserved_special_token_67|>': 128075,\n",
       " '<|reserved_special_token_68|>': 128076,\n",
       " '<|reserved_special_token_69|>': 128077,\n",
       " '<|reserved_special_token_70|>': 128078,\n",
       " '<|reserved_special_token_71|>': 128079,\n",
       " '<|reserved_special_token_72|>': 128080,\n",
       " '<|reserved_special_token_73|>': 128081,\n",
       " '<|reserved_special_token_74|>': 128082,\n",
       " '<|reserved_special_token_75|>': 128083,\n",
       " '<|reserved_special_token_76|>': 128084,\n",
       " '<|reserved_special_token_77|>': 128085,\n",
       " '<|reserved_special_token_78|>': 128086,\n",
       " '<|reserved_special_token_79|>': 128087,\n",
       " '<|reserved_special_token_80|>': 128088,\n",
       " '<|reserved_special_token_81|>': 128089,\n",
       " '<|reserved_special_token_82|>': 128090,\n",
       " '<|reserved_special_token_83|>': 128091,\n",
       " '<|reserved_special_token_84|>': 128092,\n",
       " '<|reserved_special_token_85|>': 128093,\n",
       " '<|reserved_special_token_86|>': 128094,\n",
       " '<|reserved_special_token_87|>': 128095,\n",
       " '<|reserved_special_token_88|>': 128096,\n",
       " '<|reserved_special_token_89|>': 128097,\n",
       " '<|reserved_special_token_90|>': 128098,\n",
       " '<|reserved_special_token_91|>': 128099,\n",
       " '<|reserved_special_token_92|>': 128100,\n",
       " '<|reserved_special_token_93|>': 128101,\n",
       " '<|reserved_special_token_94|>': 128102,\n",
       " '<|reserved_special_token_95|>': 128103,\n",
       " '<|reserved_special_token_96|>': 128104,\n",
       " '<|reserved_special_token_97|>': 128105,\n",
       " '<|reserved_special_token_98|>': 128106,\n",
       " '<|reserved_special_token_99|>': 128107,\n",
       " '<|reserved_special_token_100|>': 128108,\n",
       " '<|reserved_special_token_101|>': 128109,\n",
       " '<|reserved_special_token_102|>': 128110,\n",
       " '<|reserved_special_token_103|>': 128111,\n",
       " '<|reserved_special_token_104|>': 128112,\n",
       " '<|reserved_special_token_105|>': 128113,\n",
       " '<|reserved_special_token_106|>': 128114,\n",
       " '<|reserved_special_token_107|>': 128115,\n",
       " '<|reserved_special_token_108|>': 128116,\n",
       " '<|reserved_special_token_109|>': 128117,\n",
       " '<|reserved_special_token_110|>': 128118,\n",
       " '<|reserved_special_token_111|>': 128119,\n",
       " '<|reserved_special_token_112|>': 128120,\n",
       " '<|reserved_special_token_113|>': 128121,\n",
       " '<|reserved_special_token_114|>': 128122,\n",
       " '<|reserved_special_token_115|>': 128123,\n",
       " '<|reserved_special_token_116|>': 128124,\n",
       " '<|reserved_special_token_117|>': 128125,\n",
       " '<|reserved_special_token_118|>': 128126,\n",
       " '<|reserved_special_token_119|>': 128127,\n",
       " '<|reserved_special_token_120|>': 128128,\n",
       " '<|reserved_special_token_121|>': 128129,\n",
       " '<|reserved_special_token_122|>': 128130,\n",
       " '<|reserved_special_token_123|>': 128131,\n",
       " '<|reserved_special_token_124|>': 128132,\n",
       " '<|reserved_special_token_125|>': 128133,\n",
       " '<|reserved_special_token_126|>': 128134,\n",
       " '<|reserved_special_token_127|>': 128135,\n",
       " '<|reserved_special_token_128|>': 128136,\n",
       " '<|reserved_special_token_129|>': 128137,\n",
       " '<|reserved_special_token_130|>': 128138,\n",
       " '<|reserved_special_token_131|>': 128139,\n",
       " '<|reserved_special_token_132|>': 128140,\n",
       " '<|reserved_special_token_133|>': 128141,\n",
       " '<|reserved_special_token_134|>': 128142,\n",
       " '<|reserved_special_token_135|>': 128143,\n",
       " '<|reserved_special_token_136|>': 128144,\n",
       " '<|reserved_special_token_137|>': 128145,\n",
       " '<|reserved_special_token_138|>': 128146,\n",
       " '<|reserved_special_token_139|>': 128147,\n",
       " '<|reserved_special_token_140|>': 128148,\n",
       " '<|reserved_special_token_141|>': 128149,\n",
       " '<|reserved_special_token_142|>': 128150,\n",
       " '<|reserved_special_token_143|>': 128151,\n",
       " '<|reserved_special_token_144|>': 128152,\n",
       " '<|reserved_special_token_145|>': 128153,\n",
       " '<|reserved_special_token_146|>': 128154,\n",
       " '<|reserved_special_token_147|>': 128155,\n",
       " '<|reserved_special_token_148|>': 128156,\n",
       " '<|reserved_special_token_149|>': 128157,\n",
       " '<|reserved_special_token_150|>': 128158,\n",
       " '<|reserved_special_token_151|>': 128159,\n",
       " '<|reserved_special_token_152|>': 128160,\n",
       " '<|reserved_special_token_153|>': 128161,\n",
       " '<|reserved_special_token_154|>': 128162,\n",
       " '<|reserved_special_token_155|>': 128163,\n",
       " '<|reserved_special_token_156|>': 128164,\n",
       " '<|reserved_special_token_157|>': 128165,\n",
       " '<|reserved_special_token_158|>': 128166,\n",
       " '<|reserved_special_token_159|>': 128167,\n",
       " '<|reserved_special_token_160|>': 128168,\n",
       " '<|reserved_special_token_161|>': 128169,\n",
       " '<|reserved_special_token_162|>': 128170,\n",
       " '<|reserved_special_token_163|>': 128171,\n",
       " '<|reserved_special_token_164|>': 128172,\n",
       " '<|reserved_special_token_165|>': 128173,\n",
       " '<|reserved_special_token_166|>': 128174,\n",
       " '<|reserved_special_token_167|>': 128175,\n",
       " '<|reserved_special_token_168|>': 128176,\n",
       " '<|reserved_special_token_169|>': 128177,\n",
       " '<|reserved_special_token_170|>': 128178,\n",
       " '<|reserved_special_token_171|>': 128179,\n",
       " '<|reserved_special_token_172|>': 128180,\n",
       " '<|reserved_special_token_173|>': 128181,\n",
       " '<|reserved_special_token_174|>': 128182,\n",
       " '<|reserved_special_token_175|>': 128183,\n",
       " '<|reserved_special_token_176|>': 128184,\n",
       " '<|reserved_special_token_177|>': 128185,\n",
       " '<|reserved_special_token_178|>': 128186,\n",
       " '<|reserved_special_token_179|>': 128187,\n",
       " '<|reserved_special_token_180|>': 128188,\n",
       " '<|reserved_special_token_181|>': 128189,\n",
       " '<|reserved_special_token_182|>': 128190,\n",
       " '<|reserved_special_token_183|>': 128191,\n",
       " '<|reserved_special_token_184|>': 128192,\n",
       " '<|reserved_special_token_185|>': 128193,\n",
       " '<|reserved_special_token_186|>': 128194,\n",
       " '<|reserved_special_token_187|>': 128195,\n",
       " '<|reserved_special_token_188|>': 128196,\n",
       " '<|reserved_special_token_189|>': 128197,\n",
       " '<|reserved_special_token_190|>': 128198,\n",
       " '<|reserved_special_token_191|>': 128199,\n",
       " '<|reserved_special_token_192|>': 128200,\n",
       " '<|reserved_special_token_193|>': 128201,\n",
       " '<|reserved_special_token_194|>': 128202,\n",
       " '<|reserved_special_token_195|>': 128203,\n",
       " '<|reserved_special_token_196|>': 128204,\n",
       " '<|reserved_special_token_197|>': 128205,\n",
       " '<|reserved_special_token_198|>': 128206,\n",
       " '<|reserved_special_token_199|>': 128207,\n",
       " '<|reserved_special_token_200|>': 128208,\n",
       " '<|reserved_special_token_201|>': 128209,\n",
       " '<|reserved_special_token_202|>': 128210,\n",
       " '<|reserved_special_token_203|>': 128211,\n",
       " '<|reserved_special_token_204|>': 128212,\n",
       " '<|reserved_special_token_205|>': 128213,\n",
       " '<|reserved_special_token_206|>': 128214,\n",
       " '<|reserved_special_token_207|>': 128215,\n",
       " '<|reserved_special_token_208|>': 128216,\n",
       " '<|reserved_special_token_209|>': 128217,\n",
       " '<|reserved_special_token_210|>': 128218,\n",
       " '<|reserved_special_token_211|>': 128219,\n",
       " '<|reserved_special_token_212|>': 128220,\n",
       " '<|reserved_special_token_213|>': 128221,\n",
       " '<|reserved_special_token_214|>': 128222,\n",
       " '<|reserved_special_token_215|>': 128223,\n",
       " '<|reserved_special_token_216|>': 128224,\n",
       " '<|reserved_special_token_217|>': 128225,\n",
       " '<|reserved_special_token_218|>': 128226,\n",
       " '<|reserved_special_token_219|>': 128227,\n",
       " '<|reserved_special_token_220|>': 128228,\n",
       " '<|reserved_special_token_221|>': 128229,\n",
       " '<|reserved_special_token_222|>': 128230,\n",
       " '<|reserved_special_token_223|>': 128231,\n",
       " '<|reserved_special_token_224|>': 128232,\n",
       " '<|reserved_special_token_225|>': 128233,\n",
       " '<|reserved_special_token_226|>': 128234,\n",
       " '<|reserved_special_token_227|>': 128235,\n",
       " '<|reserved_special_token_228|>': 128236,\n",
       " '<|reserved_special_token_229|>': 128237,\n",
       " '<|reserved_special_token_230|>': 128238,\n",
       " '<|reserved_special_token_231|>': 128239,\n",
       " '<|reserved_special_token_232|>': 128240,\n",
       " '<|reserved_special_token_233|>': 128241,\n",
       " '<|reserved_special_token_234|>': 128242,\n",
       " '<|reserved_special_token_235|>': 128243,\n",
       " '<|reserved_special_token_236|>': 128244,\n",
       " '<|reserved_special_token_237|>': 128245,\n",
       " '<|reserved_special_token_238|>': 128246,\n",
       " '<|reserved_special_token_239|>': 128247,\n",
       " '<|reserved_special_token_240|>': 128248,\n",
       " '<|reserved_special_token_241|>': 128249,\n",
       " '<|reserved_special_token_242|>': 128250,\n",
       " '<|reserved_special_token_243|>': 128251,\n",
       " '<|reserved_special_token_244|>': 128252,\n",
       " '<|reserved_special_token_245|>': 128253,\n",
       " '<|reserved_special_token_246|>': 128254,\n",
       " '<|reserved_special_token_247|>': 128255}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.vocab\n",
    "tokenizer.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a0df67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128256"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41785563",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m prompt = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(prompt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\llm_core_self_learning\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1647\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.apply_chat_template\u001b[39m\u001b[34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[39m\n\u001b[32m   1644\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1645\u001b[39m     tokenizer_kwargs = {}\n\u001b[32m-> \u001b[39m\u001b[32m1647\u001b[39m chat_template = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(conversation, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m   1650\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(conversation[\u001b[32m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conversation[\u001b[32m0\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1651\u001b[39m ):\n\u001b[32m   1652\u001b[39m     conversations = conversation\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\llm_core_self_learning\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1825\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.get_chat_template\u001b[39m\u001b[34m(self, chat_template, tools)\u001b[39m\n\u001b[32m   1823\u001b[39m         chat_template = \u001b[38;5;28mself\u001b[39m.chat_template\n\u001b[32m   1824\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1825\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1826\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1827\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33margument was passed! For information about writing templates and setting the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1828\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1829\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1830\u001b[39m         )\n\u001b[32m   1832\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[31mValueError\u001b[39m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c2b3e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI4 = \"microsoft/Phi-4-mini-instruct\"\n",
    "DEEPSEEK = \"deepseek-ai/DeepSeek-V3.1\"\n",
    "QWEN_CODER = \"Qwen/Qwen2.5-Coder-7B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0921bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863b72496e1f46199b4217487aa9b4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7790d937204410ab42e5d763d4659d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800b8d403fb840fba6b658d3d1b75b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e348a87eaf4b4b9de44797eb3fc64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/15.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029f41e7274d44b790239f2c9429414b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/249 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79716ce21ab2418a86a0656ba580cfe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/587 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama:\n",
      "[128000, 40, 1097, 2917, 13610, 12304, 311, 1501, 473, 36368, 19109, 9857, 12509, 304, 1957, 311, 856, 445, 11237, 25175]\n",
      "['<|begin_of_text|>', 'I', ' am', ' cur', 'iously', ' excited', ' to', ' show', ' H', 'ugging', ' Face', ' Token', 'izers', ' in', ' action', ' to', ' my', ' L', 'LM', ' engineers']\n",
      "\n",
      "Phi 4:\n",
      "[40, 939, 4396, 23138, 15209, 316, 2356, 59116, 4512, 29049, 17951, 24223, 306, 3736, 316, 922, 451, 19641, 32437]\n",
      "['I', ' am', ' cur', 'iously', ' excited', ' to', ' show', ' Hug', 'ging', ' Face', ' Token', 'izers', ' in', ' action', ' to', ' my', ' L', 'LM', ' engineers']\n"
     ]
    }
   ],
   "source": [
    "phi4_tokenizer = AutoTokenizer.from_pretrained(PHI4)\n",
    "\n",
    "text = \"I am curiously excited to show Hugging Face Tokenizers in action to my LLM engineers\"\n",
    "print(\"Llama:\")\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)\n",
    "print(tokenizer.batch_decode(tokens))\n",
    "print(\"\\nPhi 4:\")\n",
    "tokens = phi4_tokenizer.encode(text)\n",
    "print(tokens)\n",
    "print(phi4_tokenizer.batch_decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2715bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Llama:\")\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "print(\"\\nPhi 4:\")\n",
    "print(phi4_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817aa41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_tokenizer = AutoTokenizer.from_pretrained(DEEPSEEK)\n",
    "\n",
    "text = \"I am curiously excited to show Hugging Face Tokenizers in action to my LLM engineers\"\n",
    "print(tokenizer.encode(text))\n",
    "print()\n",
    "print(phi4_tokenizer.encode(text))\n",
    "print()\n",
    "print(deepseek_tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533e1b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Llama:\")\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "print(\"\\nPhi:\")\n",
    "print(phi4_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "print(\"\\nDeepSeek:\")\n",
    "print(deepseek_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda92a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_tokenizer = AutoTokenizer.from_pretrained(QWEN_CODER)\n",
    "code = \"\"\"\n",
    "def hello_world(person):\n",
    "  print(\"Hello\", person)\n",
    "\"\"\"\n",
    "tokens = qwen_tokenizer.encode(code)\n",
    "for token in tokens:\n",
    "  print(f\"{token}={qwen_tokenizer.decode(token)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
